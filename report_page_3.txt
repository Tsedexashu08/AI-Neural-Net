
# Page 3: Training and Evaluation

## Section 5: Training Process

The model was trained using a comprehensive strategy to ensure robust learning and to mitigate overfitting. The `ModelTrainer` class managed the training process, which was performed on the training set (70% of the data), with performance monitored on the validation set (15%).

**Training Parameters:**

*   **Loss Function:** `sparse_categorical_crossentropy` was used, which is appropriate for multi-class classification with integer-encoded labels.
*   **Optimizer:** The **Adam optimizer** was employed with an initial learning rate of **0.001**.
*   **Batch Size:** A batch size of **32** was used, which provides a good balance between computational efficiency and stable gradient estimation.
*   **Number of Epochs:** The model was set to train for a maximum of **100 epochs**.

**Overfitting Mitigation:**

Several techniques were used to prevent the model from overfitting to the training data:

*   **Early Stopping:** An early stopping callback was configured with a patience of **15 epochs**. This callback monitors the validation loss and stops the training process if no improvement is seen for 15 consecutive epochs, restoring the weights from the best-performing epoch.
*   **Reduce Learning Rate on Plateau:** The learning rate was dynamically adjusted during training. If the validation loss did not improve for 7 epochs (half the early stopping patience), the learning rate was reduced by a factor of 0.5. This allows the model to make finer adjustments to the weights as it approaches a minimum.
*   **Dropout and L2 Regularization:** As described in the model architecture, dropout and L2 regularization were key components in preventing overfitting.

**Training Performance:**

The training process was monitored by tracking the loss and accuracy on both the training and validation sets. The `outputs/visualizations/training_history.png` plot shows these curves.

The training and validation accuracy curves track each other closely, indicating that the model is generalizing well. The loss curves show a steady decrease, and the gap between the training and validation loss is small, which further suggests that overfitting was successfully controlled. Early stopping was triggered, which prevented the model from continuing to train unnecessarily.

## Section 6: Evaluation

After training, the model's performance was evaluated on the unseen test set (15% of the data). This provides an unbiased assessment of the model's ability to generalize to new data.

**Performance Metrics:**

The model achieved a high level of performance on the test set. The key metrics from `reports/results_summary.txt` are as follows:

*   **Accuracy:** **96.7%** - This indicates that the model correctly classified the vast majority of the instances in the test set.
*   **Weighted Precision:** **96.6%**
*   **Weighted Recall:** **96.7%**
*   **Weighted F1-Score:** **96.6%**

These high scores across the board suggest that the model is both precise and robust.

**Confusion Matrix:**

The confusion matrix, visualized in `outputs/visualizations/confusion_matrix.png`, provides a detailed breakdown of the model's performance for each class. It shows the number of true positive, true negative, false positive, and false negative predictions. The strong diagonal in the matrix indicates a high number of correct predictions for all classes.

**Per-Class Metrics:**

The performance for each individual class is detailed in `outputs/visualizations/per_class_metrics.png`. This is particularly important for an imbalanced dataset. The model performs very well on the majority classes like 'Minor' and 'PDO'. For some of the minority classes, the performance is lower, which is expected. For example, the 'Fatal' class has a lower F1-score compared to the majority classes, which suggests that the model has more difficulty correctly identifying this class.

**Cross-Validation:**

The file `outputs/visualizations/cross_validation_results.png` shows the results of a 5-fold cross-validation. The consistent performance across the folds demonstrates that the model is robust and not overly sensitive to the specific train-test split. The average validation accuracy across the folds is high, which further validates the model's performance.
