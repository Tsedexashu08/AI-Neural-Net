
# Page 2: Data Preparation and Model Architecture

## Section 3: Data Preparation

The raw dataset required several preprocessing steps to be suitable for training a neural network. The following steps were implemented using the `DataPreprocessor` class.

**Preprocessing Steps:**

*   **Missing Value Handling:**
    *   Columns with more than 30% missing data were dropped. This included columns like 'Zone', 'Driving License', and several 'Victim' related features.
    *   For the remaining columns, missing numeric values were imputed using the **median**, while missing categorical values were filled with the **mode**. This approach is robust to outliers and prevents data loss.

*   **Categorical Variable Encoding:**
    *   A mixed-encoding strategy was used to handle the categorical features:
        *   **Binary features** (2 unique values) were encoded using **Label Encoding**.
        *   **Low-cardinality features** (up to 10 unique values) were transformed using **One-Hot Encoding**. This creates new binary columns for each category, avoiding an arbitrary order.
        *   **High-cardinality features** (more than 10 unique values) were encoded using **Frequency Encoding**. This replaces each category with its frequency of occurrence, which can be a useful signal for the model.

*   **Feature Scaling:**
    *   All numeric features were scaled using **StandardScaler**. This standardizes the features to have a mean of 0 and a standard deviation of 1, ensuring that all features contribute equally to the model's training process.

*   **Outlier Handling:**
    *   Outliers in numeric features were handled by **capping** them at the 1.5 * IQR (Interquartile Range) bounds. This reduces the skewing effect of extreme values without removing the data points entirely.

**Data Split:**

The preprocessed dataset was split into three sets to ensure a robust evaluation of the model:
*   **Training Set:** 70% of the data, used to train the model.
*   **Validation Set:** 15% of the data, used to tune hyperparameters and monitor for overfitting during training.
*   **Test Set:** 15% of the data, kept separate and used for the final evaluation of the model's performance.

The split was performed with stratification on the target variable ('Accident Type') to ensure that the class distribution was preserved across all three sets. The processed data files are stored in the `outputs/processed_data/` directory.

## Section 4: Model Architecture

The neural network was designed to be a deep, regularized model suitable for this classification task. The architecture was built using the `ModelBuilder` class in TensorFlow/Keras.

**Network Design:**

The model consists of the following layers:

*   **Input Layer:** The input layer accepts a vector of size corresponding to the number of features after preprocessing.
*   **Hidden Layers:** The network has **three hidden layers** with a decreasing number of neurons: **64, 32, and 16**. This "funnel" structure is designed to learn hierarchical representations of the data, from general features in the first layer to more specific patterns in the deeper layers.
    *   **Activation Function:** The **ReLU (Rectified Linear Unit)** activation function is used in all hidden layers. ReLU is computationally efficient and helps mitigate the vanishing gradient problem.
*   **Output Layer:**
    *   The output layer has a number of neurons equal to the number of classes in the target variable.
    *   The **Softmax** activation function is used, which outputs a probability distribution over the classes, making it ideal for multi-class classification.

**Justification for Design Choices:**

*   **Depth and Width:** The choice of three hidden layers provides a good balance between model capacity and the risk of overfitting. The decreasing number of neurons helps in compressing the information and extracting the most salient features.
*   **Regularization:** To prevent overfitting, several regularization techniques were employed:
    *   **L2 Regularization (Î»=0.001):** Applied to the weights of the dense layers to penalize large weights and encourage the model to learn simpler patterns.
    *   **Dropout (Rate=0.3):** Applied after each hidden layer to randomly set a fraction of neuron activations to zero during training. This forces the network to learn more robust features.
    *   **Batch Normalization:** Used after each dense layer (except the last one) to stabilize the learning process and speed up convergence by normalizing the inputs to each layer.
*   **Optimizer and Loss Function:**
    *   The **Adam optimizer** was chosen for its adaptive learning rate capabilities, which performs well in most scenarios.
    *   The **sparse_categorical_crossentropy** loss function was used, as it is designed for multi-class classification problems where the labels are provided as integers.

This architecture is designed to be powerful enough to capture the complex relationships in the data while being regularized enough to generalize well to unseen data.
