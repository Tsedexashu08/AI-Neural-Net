
# Slide 6: Model Architecture

**(Center of the slide: A diagram of the neural network architecture)**

**Network Design:**
*   **Input Layer:** Takes the preprocessed feature vector.
*   **Hidden Layers:** 3 Dense layers with 64, 32, and 16 neurons.
    *   **Activation:** ReLU
*   **Output Layer:**
    *   **Activation:** Softmax (for multi-class classification)

**Regularization:**
*   **L2 Regularization (Î»=0.001)**
*   **Dropout (Rate=0.3)**
*   **Batch Normalization**

---
**Slide Notes:**

*   Now let's look at the heart of our project: the neural network architecture.
*   The project utilizes a Feedforward Neural Network (FNN), specifically a Multi-layer Perceptron (MLP). This architecture features sequential, densely connected
    layers, with information flowing unidirectionally from input to output
*   We designed a 3-layer deep neural network. The number of neurons decreases in each layer, which helps the network to learn more abstract and compressed representations of the data.
*   We used the ReLU activation function in the hidden layers, which is a standard and effective choice for deep learning models.
*   The output layer uses a Softmax activation function to output a probability for each of the accident types.
*   To prevent our model from overfitting, we used a combination of three regularization techniques: L2 regularization, which penalizes large weights; Dropout, which randomly deactivates neurons during training; and Batch Normalization, which helps to stabilize the learning process.
