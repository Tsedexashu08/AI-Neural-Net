
# Page 1: Introduction and Data Understanding

## Section 1: Introduction

**Objective:**
The primary objective of this project is to develop a robust neural network-based classification model. This model will be trained on the provided `CrashData.xlsx` dataset to predict the 'Accident Type' based on various contributing factors. The project encompasses the entire machine learning workflow, from data preprocessing and exploratory data analysis to model design, training, evaluation, and interpretation of the results.

**Dataset:**
The dataset used for this project is `CrashData.xlsx`, which contains information about vehicle crashes. It includes a wide range of features such as driver details, vehicle information, road conditions, and environmental factors.

**Methodology:**
The project follows a structured methodology:
1.  **Data Understanding:** Initial exploration of the dataset to understand its structure, identify data quality issues, and uncover insights.
2.  **Data Preparation:** Cleaning and preprocessing the data to make it suitable for a neural network model. This includes handling missing values, encoding categorical features, and scaling numeric features.
3.  **Model Design:** Architecting a neural network with appropriate layers, neurons, and activation functions.
4.  **Training:** Training the model on the preprocessed data, using techniques to monitor performance and prevent overfitting.
5.  **Evaluation:** Assessing the model's performance on a separate test set using various classification metrics.
6.  **Interpretation:** Analyzing the results to understand the model's strengths and weaknesses and suggest areas for improvement.

## Section 2: Data Understanding

**Initial Findings:**
A preliminary analysis of the dataset was conducted to understand its basic characteristics.

*   **Structure:** The dataset originally contains **60,004 rows** and **54 columns**. The features are a mix of numeric (22) and categorical (31) data types, providing a comprehensive view of each incident. The memory usage of the dataset is approximately **102.18 MB**.

*   **Missing Values:** The dataset contains a significant number of missing values. Several columns, such as 'Zone', 'Driving License', 'Exiting/entering', 'Contributory Action', and 'Region', are **100% empty** and were dropped. Other columns like 'Victim-3 Movement' (99.8% missing) and 'Victim-2 Movement' (99.5% missing) also have very high percentages of missing data. A strategy of dropping columns with more than 30% missing data was adopted. For the rest, imputation was performed using the median for numerical features and the mode for categorical features.

*   **Duplicates:** The dataset was checked for duplicate rows, and **no duplicate entries** were found. This indicates good data integrity in terms of redundant records.

**Visualizations:**

*   **Class Distribution:** The distribution of the target variable, 'Accident Type', is imbalanced. This is a crucial observation as it can bias the model towards the majority class. The visualization `outputs/visualizations/class_distribution.png` illustrates this imbalance, which needs to be addressed during the modeling phase, possibly by using techniques like stratified sampling or class weights.

*   **Feature Distributions:** The distributions of various numeric and categorical features were examined using histograms and bar charts, available in `outputs/visualizations/feature_distributions.png`. This helped in understanding the range and frequency of different values for each feature.

*   **Correlation Analysis:** A correlation heatmap (`outputs/visualizations/correlation_heatmap.png`) was generated to understand the relationships between numeric features. The top correlations are listed in `reports/top_correlations.csv`. A notable correlation of **0.63** was found between 'Driver age' and 'Driver experiance(years)', which is expected.

*   **Outliers:** Boxplots (`outputs/visualizations/boxplots_outliers.png`) were used to identify outliers in the numeric features. Outliers can skew the model's learning process, and a capping strategy using the Interquartile Range (IQR) was applied to handle them.

**Preprocessing Requirements:**
Based on the initial data exploration, the following preprocessing steps were identified as necessary:
*   **Handling Missing Values:** Dropping columns with high percentages of missing data and imputing the rest.
*   **Encoding Categorical Variables:** Converting categorical features into a numeric format using one-hot encoding, label encoding, or frequency encoding based on the cardinality of the feature.
*   **Feature Scaling:** Normalizing the numeric features to a standard scale (e.g., using StandardScaler) to ensure that features with larger ranges do not dominate the learning process.
*   **Handling Imbalance:** Using stratified splitting to maintain the class distribution in the train, validation, and test sets.
*   **Outlier Treatment:** Capping outliers to reduce their influence on the model.
